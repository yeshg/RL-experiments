{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Policy Gradients for Cartpole Gym Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we'll implement an agent <b>that plays Cartpole </b>\n",
    "\n",
    "<img src=\"http://neuro-educator.com/wp-content/uploads/2017/09/DQN.gif\" alt=\"Cartpole gif\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf    # deep learning framework\n",
    "import numpy as np         # used for matrix operations\n",
    "import gym                 # used for getting the cartpole environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env = env.unwrapped        # see gym documentation from openai.org\n",
    "# Policy gradient has high variance, set seed for reproducability\n",
    "env.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evironment Hyperparameters\n",
    "state_size = 4\n",
    "action_size = env.action_space.n # get the number of possible actions\n",
    "\n",
    "## Training Hyperparameters\n",
    "max_episodes = 300 # increase if using gpu\n",
    "learning_rate = 0.01 # learning rate alpha\n",
    "gamma = 0.95 # discount rate gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Function to apply discount and return normalized rewards at end of an episode\n",
    "This is necessary because we are using a Monte Carlo approach (so rewards are discounted after a full episode is complete and then policy is updated). If using a continuous approach (such as Temporal Difference or TD-learning) then the discount and update steps happen after each action leads to a new state.\n",
    "\n",
    "The discounted cumulative expected reward is important to make sure that predictable rewards from the beginning of the game don't dominate over the less predictable long term future rewards. gamma is the relevant hyperparameter.\n",
    "* if gamma is large the discount is small, so agent cares more about long term reward\n",
    "* if gamma is small, discount is large, so agent cares more about short term reward\n",
    "\n",
    "To calculate the discounted cumulative expected reward, simply sum all previous rewards multipled (discounted) by gamma to the exponent of the time step:\n",
    "\n",
    "$$\n",
    "\\sum_{k=0}^{\\infty }\\gamma^{k}R_{t+k+1}\\;where\\;\n",
    "\\gamma\\;\\epsilon\\;[0,1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_and_normalize_rewards(episode_rewards):\n",
    "    discounted_episode_rewards = np.zeros_like(episode_rewards)\n",
    "    cumulative = 0.0\n",
    "    for i in reversed(range(len(episode_rewards))):\n",
    "        cumulative = cumulative * gamma + episode_rewards[i]\n",
    "        discounted_episode_rewards[i] = cumulative\n",
    "    \n",
    "    mean = np.mean(discounted_episode_rewards)\n",
    "    std = np.std(discounted_episode_rewards)\n",
    "    discounted_episode_rewards = (discounted_episode_rewards - mean) / (std)\n",
    "    \n",
    "    return discounted_episode_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Neural Network Model\n",
    "NN Input: the current state (an array of 4 values)\n",
    "\n",
    "NN Architecture: 3 fully connected layers, ReLU activation function. Output activation function is softmax used to make output a probability distribution.\n",
    "\n",
    "NN Output: Action Distrubtion generated from softmax and NN processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"inputs\"):\n",
    "    input_ = tf.placeholder(tf.float32, [None, state_size], name=\"input_\")\n",
    "    actions = tf.placeholder(tf.int32, [None, action_size], name=\"actions\")\n",
    "    discounted_episode_rewards_ = tf.placeholder(tf.float32, [None,], name=\"discounted_episode_rewards\")\n",
    "    \n",
    "    # Add this placeholder for having this variable in tensorboard\n",
    "    mean_reward_ = tf.placeholder(tf.float32 , name=\"mean_reward\")\n",
    "\n",
    "    with tf.name_scope(\"fc1\"):\n",
    "        fc1 = tf.contrib.layers.fully_connected(inputs = input_,\n",
    "                                                num_outputs = 10,\n",
    "                                                activation_fn=tf.nn.relu,\n",
    "                                                weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "    with tf.name_scope(\"fc2\"):\n",
    "        fc2 = tf.contrib.layers.fully_connected(inputs = fc1,\n",
    "                                                num_outputs = action_size,\n",
    "                                                activation_fn= tf.nn.relu,\n",
    "                                                weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    with tf.name_scope(\"fc3\"):\n",
    "        fc3 = tf.contrib.layers.fully_connected(inputs = fc2,\n",
    "                                                num_outputs = action_size,\n",
    "                                                activation_fn= None,\n",
    "                                                weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "    with tf.name_scope(\"softmax\"):\n",
    "        action_distribution = tf.nn.softmax(fc3)\n",
    "\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        # tf.nn.softmax_cross_entropy_with_logits computes the cross entropy of the result after applying the softmax function\n",
    "        # If you have single-class labels, where an object can only belong to one class, you might now consider using \n",
    "        # tf.nn.sparse_softmax_cross_entropy_with_logits so that you don't have to convert your labels to a dense one-hot array. \n",
    "        neg_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(logits = fc3, labels = actions)\n",
    "        loss = tf.reduce_mean(neg_log_prob * discounted_episode_rewards_) \n",
    "        \n",
    "    \n",
    "    with tf.name_scope(\"train\"):\n",
    "        train_opt = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Tensorboard\n",
    "Important metrics for RL are Loss (negative score) and the mean of the reward.\n",
    "\n",
    "To launch tensorboard: ```tensorboard --logdir=<path_specified_below>```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup TensorBoard Writer\n",
    "writer = tf.summary.FileWriter(\"~/tensorboard/pg/1\") # change to any path as long as user has access\n",
    "\n",
    "## Losses\n",
    "tf.summary.scalar(\"Loss\", loss)\n",
    "\n",
    "## Reward mean\n",
    "tf.summary.scalar(\"Reward_mean\", mean_reward_)\n",
    "\n",
    "write_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train Agent\n",
    "Pseudocode:\n",
    "```\n",
    "Create the network\n",
    "maxReward = 0 # initialize and keep track of max reward\n",
    "for episode in range(max_episodes):\n",
    "    episode + 1\n",
    "    reset environment\n",
    "    reset stores (states, actions, rewards)\n",
    "    \n",
    "    for each step:\n",
    "        choose action a\n",
    "        perform action a and get r\n",
    "        store s, a, r\n",
    "        check if done:\n",
    "            calculate sum reward\n",
    "            calculate gamma Gt\n",
    "            optimize max of score (min of loss or min of negative score)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Episode:  0\n",
      "Reward:  21.0\n",
      "Mean Reward 21.0\n",
      "Max reward so far:  21.0\n",
      "Model saved\n",
      "==========================================\n",
      "Episode:  1\n",
      "Reward:  14.0\n",
      "Mean Reward 17.5\n",
      "Max reward so far:  21.0\n",
      "==========================================\n",
      "Episode:  2\n",
      "Reward:  18.0\n",
      "Mean Reward 17.666666666666668\n",
      "Max reward so far:  21.0\n",
      "==========================================\n",
      "Episode:  3\n",
      "Reward:  19.0\n",
      "Mean Reward 18.0\n",
      "Max reward so far:  21.0\n",
      "==========================================\n",
      "Episode:  4\n",
      "Reward:  14.0\n",
      "Mean Reward 17.2\n",
      "Max reward so far:  21.0\n",
      "==========================================\n",
      "Episode:  5\n",
      "Reward:  11.0\n",
      "Mean Reward 16.166666666666668\n",
      "Max reward so far:  21.0\n",
      "==========================================\n",
      "Episode:  6\n",
      "Reward:  11.0\n",
      "Mean Reward 15.428571428571429\n",
      "Max reward so far:  21.0\n",
      "==========================================\n",
      "Episode:  7\n",
      "Reward:  67.0\n",
      "Mean Reward 21.875\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  8\n",
      "Reward:  12.0\n",
      "Mean Reward 20.77777777777778\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  9\n",
      "Reward:  21.0\n",
      "Mean Reward 20.8\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  10\n",
      "Reward:  17.0\n",
      "Mean Reward 20.454545454545453\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  11\n",
      "Reward:  28.0\n",
      "Mean Reward 21.083333333333332\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  12\n",
      "Reward:  12.0\n",
      "Mean Reward 20.384615384615383\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  13\n",
      "Reward:  11.0\n",
      "Mean Reward 19.714285714285715\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  14\n",
      "Reward:  25.0\n",
      "Mean Reward 20.066666666666666\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  15\n",
      "Reward:  29.0\n",
      "Mean Reward 20.625\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  16\n",
      "Reward:  24.0\n",
      "Mean Reward 20.823529411764707\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  17\n",
      "Reward:  10.0\n",
      "Mean Reward 20.22222222222222\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  18\n",
      "Reward:  9.0\n",
      "Mean Reward 19.63157894736842\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  19\n",
      "Reward:  14.0\n",
      "Mean Reward 19.35\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  20\n",
      "Reward:  21.0\n",
      "Mean Reward 19.428571428571427\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  21\n",
      "Reward:  18.0\n",
      "Mean Reward 19.363636363636363\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  22\n",
      "Reward:  20.0\n",
      "Mean Reward 19.391304347826086\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  23\n",
      "Reward:  20.0\n",
      "Mean Reward 19.416666666666668\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  24\n",
      "Reward:  13.0\n",
      "Mean Reward 19.16\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  25\n",
      "Reward:  13.0\n",
      "Mean Reward 18.923076923076923\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  26\n",
      "Reward:  26.0\n",
      "Mean Reward 19.185185185185187\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  27\n",
      "Reward:  9.0\n",
      "Mean Reward 18.821428571428573\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  28\n",
      "Reward:  22.0\n",
      "Mean Reward 18.93103448275862\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  29\n",
      "Reward:  14.0\n",
      "Mean Reward 18.766666666666666\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  30\n",
      "Reward:  9.0\n",
      "Mean Reward 18.451612903225808\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  31\n",
      "Reward:  28.0\n",
      "Mean Reward 18.75\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  32\n",
      "Reward:  41.0\n",
      "Mean Reward 19.424242424242426\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  33\n",
      "Reward:  24.0\n",
      "Mean Reward 19.558823529411764\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  34\n",
      "Reward:  14.0\n",
      "Mean Reward 19.4\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  35\n",
      "Reward:  14.0\n",
      "Mean Reward 19.25\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  36\n",
      "Reward:  13.0\n",
      "Mean Reward 19.08108108108108\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  37\n",
      "Reward:  28.0\n",
      "Mean Reward 19.31578947368421\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  38\n",
      "Reward:  12.0\n",
      "Mean Reward 19.128205128205128\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  39\n",
      "Reward:  12.0\n",
      "Mean Reward 18.95\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  40\n",
      "Reward:  13.0\n",
      "Mean Reward 18.804878048780488\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  41\n",
      "Reward:  11.0\n",
      "Mean Reward 18.61904761904762\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  42\n",
      "Reward:  29.0\n",
      "Mean Reward 18.86046511627907\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  43\n",
      "Reward:  15.0\n",
      "Mean Reward 18.772727272727273\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  44\n",
      "Reward:  18.0\n",
      "Mean Reward 18.755555555555556\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  45\n",
      "Reward:  29.0\n",
      "Mean Reward 18.97826086956522\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  46\n",
      "Reward:  31.0\n",
      "Mean Reward 19.23404255319149\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  47\n",
      "Reward:  9.0\n",
      "Mean Reward 19.020833333333332\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  48\n",
      "Reward:  18.0\n",
      "Mean Reward 19.0\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  49\n",
      "Reward:  15.0\n",
      "Mean Reward 18.92\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  50\n",
      "Reward:  19.0\n",
      "Mean Reward 18.92156862745098\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  51\n",
      "Reward:  16.0\n",
      "Mean Reward 18.865384615384617\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  52\n",
      "Reward:  11.0\n",
      "Mean Reward 18.71698113207547\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  53\n",
      "Reward:  16.0\n",
      "Mean Reward 18.666666666666668\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  54\n",
      "Reward:  12.0\n",
      "Mean Reward 18.545454545454547\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  55\n",
      "Reward:  24.0\n",
      "Mean Reward 18.642857142857142\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  56\n",
      "Reward:  11.0\n",
      "Mean Reward 18.50877192982456\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  57\n",
      "Reward:  19.0\n",
      "Mean Reward 18.517241379310345\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  58\n",
      "Reward:  14.0\n",
      "Mean Reward 18.440677966101696\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  59\n",
      "Reward:  28.0\n",
      "Mean Reward 18.6\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  60\n",
      "Reward:  39.0\n",
      "Mean Reward 18.934426229508198\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  61\n",
      "Reward:  9.0\n",
      "Mean Reward 18.774193548387096\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  62\n",
      "Reward:  12.0\n",
      "Mean Reward 18.666666666666668\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  63\n",
      "Reward:  18.0\n",
      "Mean Reward 18.65625\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  64\n",
      "Reward:  15.0\n",
      "Mean Reward 18.6\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  65\n",
      "Reward:  15.0\n",
      "Mean Reward 18.545454545454547\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  66\n",
      "Reward:  17.0\n",
      "Mean Reward 18.52238805970149\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  67\n",
      "Reward:  10.0\n",
      "Mean Reward 18.397058823529413\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  68\n",
      "Reward:  12.0\n",
      "Mean Reward 18.304347826086957\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  69\n",
      "Reward:  12.0\n",
      "Mean Reward 18.214285714285715\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  70\n",
      "Reward:  12.0\n",
      "Mean Reward 18.12676056338028\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  71\n",
      "Reward:  14.0\n",
      "Mean Reward 18.069444444444443\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  72\n",
      "Reward:  19.0\n",
      "Mean Reward 18.08219178082192\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  73\n",
      "Reward:  63.0\n",
      "Mean Reward 18.68918918918919\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  74\n",
      "Reward:  25.0\n",
      "Mean Reward 18.773333333333333\n",
      "Max reward so far:  67.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Episode:  75\n",
      "Reward:  55.0\n",
      "Mean Reward 19.25\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  76\n",
      "Reward:  9.0\n",
      "Mean Reward 19.116883116883116\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  77\n",
      "Reward:  29.0\n",
      "Mean Reward 19.243589743589745\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  78\n",
      "Reward:  14.0\n",
      "Mean Reward 19.17721518987342\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  79\n",
      "Reward:  24.0\n",
      "Mean Reward 19.2375\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  80\n",
      "Reward:  30.0\n",
      "Mean Reward 19.37037037037037\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  81\n",
      "Reward:  13.0\n",
      "Mean Reward 19.29268292682927\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  82\n",
      "Reward:  19.0\n",
      "Mean Reward 19.289156626506024\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  83\n",
      "Reward:  15.0\n",
      "Mean Reward 19.238095238095237\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  84\n",
      "Reward:  28.0\n",
      "Mean Reward 19.341176470588234\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  85\n",
      "Reward:  17.0\n",
      "Mean Reward 19.313953488372093\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  86\n",
      "Reward:  12.0\n",
      "Mean Reward 19.229885057471265\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  87\n",
      "Reward:  18.0\n",
      "Mean Reward 19.21590909090909\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  88\n",
      "Reward:  17.0\n",
      "Mean Reward 19.191011235955056\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  89\n",
      "Reward:  26.0\n",
      "Mean Reward 19.266666666666666\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  90\n",
      "Reward:  19.0\n",
      "Mean Reward 19.263736263736263\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  91\n",
      "Reward:  23.0\n",
      "Mean Reward 19.304347826086957\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  92\n",
      "Reward:  56.0\n",
      "Mean Reward 19.698924731182796\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  93\n",
      "Reward:  13.0\n",
      "Mean Reward 19.627659574468087\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  94\n",
      "Reward:  17.0\n",
      "Mean Reward 19.6\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  95\n",
      "Reward:  28.0\n",
      "Mean Reward 19.6875\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  96\n",
      "Reward:  12.0\n",
      "Mean Reward 19.608247422680414\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  97\n",
      "Reward:  15.0\n",
      "Mean Reward 19.56122448979592\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  98\n",
      "Reward:  17.0\n",
      "Mean Reward 19.535353535353536\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  99\n",
      "Reward:  59.0\n",
      "Mean Reward 19.93\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  100\n",
      "Reward:  11.0\n",
      "Mean Reward 19.84158415841584\n",
      "Max reward so far:  67.0\n",
      "Model saved\n",
      "==========================================\n",
      "Episode:  101\n",
      "Reward:  19.0\n",
      "Mean Reward 19.833333333333332\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  102\n",
      "Reward:  13.0\n",
      "Mean Reward 19.766990291262136\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  103\n",
      "Reward:  13.0\n",
      "Mean Reward 19.701923076923077\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  104\n",
      "Reward:  40.0\n",
      "Mean Reward 19.895238095238096\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  105\n",
      "Reward:  31.0\n",
      "Mean Reward 20.0\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  106\n",
      "Reward:  15.0\n",
      "Mean Reward 19.953271028037385\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  107\n",
      "Reward:  33.0\n",
      "Mean Reward 20.074074074074073\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  108\n",
      "Reward:  24.0\n",
      "Mean Reward 20.110091743119266\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  109\n",
      "Reward:  11.0\n",
      "Mean Reward 20.027272727272727\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  110\n",
      "Reward:  28.0\n",
      "Mean Reward 20.0990990990991\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  111\n",
      "Reward:  15.0\n",
      "Mean Reward 20.053571428571427\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  112\n",
      "Reward:  10.0\n",
      "Mean Reward 19.964601769911503\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  113\n",
      "Reward:  11.0\n",
      "Mean Reward 19.885964912280702\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  114\n",
      "Reward:  12.0\n",
      "Mean Reward 19.817391304347826\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  115\n",
      "Reward:  17.0\n",
      "Mean Reward 19.79310344827586\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  116\n",
      "Reward:  57.0\n",
      "Mean Reward 20.11111111111111\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  117\n",
      "Reward:  29.0\n",
      "Mean Reward 20.1864406779661\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  118\n",
      "Reward:  32.0\n",
      "Mean Reward 20.285714285714285\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  119\n",
      "Reward:  15.0\n",
      "Mean Reward 20.241666666666667\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  120\n",
      "Reward:  19.0\n",
      "Mean Reward 20.231404958677686\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  121\n",
      "Reward:  31.0\n",
      "Mean Reward 20.31967213114754\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  122\n",
      "Reward:  15.0\n",
      "Mean Reward 20.276422764227643\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  123\n",
      "Reward:  12.0\n",
      "Mean Reward 20.20967741935484\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  124\n",
      "Reward:  36.0\n",
      "Mean Reward 20.336\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  125\n",
      "Reward:  26.0\n",
      "Mean Reward 20.38095238095238\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  126\n",
      "Reward:  14.0\n",
      "Mean Reward 20.330708661417322\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  127\n",
      "Reward:  12.0\n",
      "Mean Reward 20.265625\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  128\n",
      "Reward:  16.0\n",
      "Mean Reward 20.232558139534884\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  129\n",
      "Reward:  12.0\n",
      "Mean Reward 20.16923076923077\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  130\n",
      "Reward:  17.0\n",
      "Mean Reward 20.14503816793893\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  131\n",
      "Reward:  17.0\n",
      "Mean Reward 20.12121212121212\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  132\n",
      "Reward:  31.0\n",
      "Mean Reward 20.203007518796994\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  133\n",
      "Reward:  36.0\n",
      "Mean Reward 20.32089552238806\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  134\n",
      "Reward:  20.0\n",
      "Mean Reward 20.31851851851852\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  135\n",
      "Reward:  10.0\n",
      "Mean Reward 20.24264705882353\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  136\n",
      "Reward:  12.0\n",
      "Mean Reward 20.182481751824817\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  137\n",
      "Reward:  21.0\n",
      "Mean Reward 20.18840579710145\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  138\n",
      "Reward:  19.0\n",
      "Mean Reward 20.179856115107913\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  139\n",
      "Reward:  22.0\n",
      "Mean Reward 20.192857142857143\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  140\n",
      "Reward:  52.0\n",
      "Mean Reward 20.418439716312058\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  141\n",
      "Reward:  15.0\n",
      "Mean Reward 20.380281690140844\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  142\n",
      "Reward:  18.0\n",
      "Mean Reward 20.363636363636363\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  143\n",
      "Reward:  19.0\n",
      "Mean Reward 20.354166666666668\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  144\n",
      "Reward:  18.0\n",
      "Mean Reward 20.337931034482757\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  145\n",
      "Reward:  18.0\n",
      "Mean Reward 20.32191780821918\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  146\n",
      "Reward:  18.0\n",
      "Mean Reward 20.306122448979593\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  147\n",
      "Reward:  19.0\n",
      "Mean Reward 20.2972972972973\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  148\n",
      "Reward:  26.0\n",
      "Mean Reward 20.335570469798657\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  149\n",
      "Reward:  12.0\n",
      "Mean Reward 20.28\n",
      "Max reward so far:  67.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Episode:  150\n",
      "Reward:  17.0\n",
      "Mean Reward 20.258278145695364\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  151\n",
      "Reward:  11.0\n",
      "Mean Reward 20.19736842105263\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  152\n",
      "Reward:  10.0\n",
      "Mean Reward 20.130718954248366\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  153\n",
      "Reward:  23.0\n",
      "Mean Reward 20.149350649350648\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  154\n",
      "Reward:  45.0\n",
      "Mean Reward 20.309677419354838\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  155\n",
      "Reward:  11.0\n",
      "Mean Reward 20.25\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  156\n",
      "Reward:  14.0\n",
      "Mean Reward 20.21019108280255\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  157\n",
      "Reward:  17.0\n",
      "Mean Reward 20.189873417721518\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  158\n",
      "Reward:  20.0\n",
      "Mean Reward 20.18867924528302\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  159\n",
      "Reward:  26.0\n",
      "Mean Reward 20.225\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  160\n",
      "Reward:  9.0\n",
      "Mean Reward 20.15527950310559\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  161\n",
      "Reward:  18.0\n",
      "Mean Reward 20.141975308641975\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  162\n",
      "Reward:  16.0\n",
      "Mean Reward 20.116564417177916\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  163\n",
      "Reward:  27.0\n",
      "Mean Reward 20.158536585365855\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  164\n",
      "Reward:  29.0\n",
      "Mean Reward 20.21212121212121\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  165\n",
      "Reward:  21.0\n",
      "Mean Reward 20.216867469879517\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  166\n",
      "Reward:  26.0\n",
      "Mean Reward 20.251497005988025\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  167\n",
      "Reward:  11.0\n",
      "Mean Reward 20.196428571428573\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  168\n",
      "Reward:  24.0\n",
      "Mean Reward 20.218934911242602\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  169\n",
      "Reward:  16.0\n",
      "Mean Reward 20.194117647058825\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  170\n",
      "Reward:  19.0\n",
      "Mean Reward 20.18713450292398\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  171\n",
      "Reward:  22.0\n",
      "Mean Reward 20.197674418604652\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  172\n",
      "Reward:  10.0\n",
      "Mean Reward 20.13872832369942\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  173\n",
      "Reward:  26.0\n",
      "Mean Reward 20.17241379310345\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  174\n",
      "Reward:  55.0\n",
      "Mean Reward 20.37142857142857\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  175\n",
      "Reward:  21.0\n",
      "Mean Reward 20.375\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  176\n",
      "Reward:  14.0\n",
      "Mean Reward 20.338983050847457\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  177\n",
      "Reward:  16.0\n",
      "Mean Reward 20.314606741573034\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  178\n",
      "Reward:  21.0\n",
      "Mean Reward 20.318435754189945\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  179\n",
      "Reward:  14.0\n",
      "Mean Reward 20.283333333333335\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  180\n",
      "Reward:  19.0\n",
      "Mean Reward 20.276243093922652\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  181\n",
      "Reward:  20.0\n",
      "Mean Reward 20.274725274725274\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  182\n",
      "Reward:  17.0\n",
      "Mean Reward 20.256830601092897\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  183\n",
      "Reward:  15.0\n",
      "Mean Reward 20.22826086956522\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  184\n",
      "Reward:  50.0\n",
      "Mean Reward 20.38918918918919\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  185\n",
      "Reward:  37.0\n",
      "Mean Reward 20.478494623655912\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  186\n",
      "Reward:  13.0\n",
      "Mean Reward 20.43850267379679\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  187\n",
      "Reward:  15.0\n",
      "Mean Reward 20.409574468085108\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  188\n",
      "Reward:  17.0\n",
      "Mean Reward 20.391534391534393\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  189\n",
      "Reward:  11.0\n",
      "Mean Reward 20.342105263157894\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  190\n",
      "Reward:  44.0\n",
      "Mean Reward 20.465968586387433\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  191\n",
      "Reward:  11.0\n",
      "Mean Reward 20.416666666666668\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  192\n",
      "Reward:  17.0\n",
      "Mean Reward 20.39896373056995\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  193\n",
      "Reward:  14.0\n",
      "Mean Reward 20.3659793814433\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  194\n",
      "Reward:  46.0\n",
      "Mean Reward 20.497435897435896\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  195\n",
      "Reward:  29.0\n",
      "Mean Reward 20.540816326530614\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  196\n",
      "Reward:  29.0\n",
      "Mean Reward 20.583756345177665\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  197\n",
      "Reward:  18.0\n",
      "Mean Reward 20.57070707070707\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  198\n",
      "Reward:  34.0\n",
      "Mean Reward 20.63819095477387\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  199\n",
      "Reward:  11.0\n",
      "Mean Reward 20.59\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  200\n",
      "Reward:  15.0\n",
      "Mean Reward 20.562189054726367\n",
      "Max reward so far:  67.0\n",
      "Model saved\n",
      "==========================================\n",
      "Episode:  201\n",
      "Reward:  16.0\n",
      "Mean Reward 20.53960396039604\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  202\n",
      "Reward:  16.0\n",
      "Mean Reward 20.517241379310345\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  203\n",
      "Reward:  14.0\n",
      "Mean Reward 20.485294117647058\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  204\n",
      "Reward:  11.0\n",
      "Mean Reward 20.4390243902439\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  205\n",
      "Reward:  17.0\n",
      "Mean Reward 20.42233009708738\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  206\n",
      "Reward:  9.0\n",
      "Mean Reward 20.367149758454108\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  207\n",
      "Reward:  20.0\n",
      "Mean Reward 20.365384615384617\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  208\n",
      "Reward:  21.0\n",
      "Mean Reward 20.36842105263158\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  209\n",
      "Reward:  32.0\n",
      "Mean Reward 20.423809523809524\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  210\n",
      "Reward:  13.0\n",
      "Mean Reward 20.38862559241706\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  211\n",
      "Reward:  11.0\n",
      "Mean Reward 20.34433962264151\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  212\n",
      "Reward:  25.0\n",
      "Mean Reward 20.366197183098592\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  213\n",
      "Reward:  20.0\n",
      "Mean Reward 20.364485981308412\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  214\n",
      "Reward:  13.0\n",
      "Mean Reward 20.330232558139535\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  215\n",
      "Reward:  11.0\n",
      "Mean Reward 20.287037037037038\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  216\n",
      "Reward:  19.0\n",
      "Mean Reward 20.28110599078341\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  217\n",
      "Reward:  12.0\n",
      "Mean Reward 20.243119266055047\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  218\n",
      "Reward:  42.0\n",
      "Mean Reward 20.34246575342466\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  219\n",
      "Reward:  16.0\n",
      "Mean Reward 20.322727272727274\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  220\n",
      "Reward:  20.0\n",
      "Mean Reward 20.32126696832579\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  221\n",
      "Reward:  22.0\n",
      "Mean Reward 20.32882882882883\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  222\n",
      "Reward:  16.0\n",
      "Mean Reward 20.309417040358746\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  223\n",
      "Reward:  10.0\n",
      "Mean Reward 20.263392857142858\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  224\n",
      "Reward:  14.0\n",
      "Mean Reward 20.235555555555557\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  225\n",
      "Reward:  11.0\n",
      "Mean Reward 20.194690265486727\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  226\n",
      "Reward:  9.0\n",
      "Mean Reward 20.145374449339208\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  227\n",
      "Reward:  16.0\n",
      "Mean Reward 20.12719298245614\n",
      "Max reward so far:  67.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Episode:  228\n",
      "Reward:  19.0\n",
      "Mean Reward 20.12227074235808\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  229\n",
      "Reward:  27.0\n",
      "Mean Reward 20.152173913043477\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  230\n",
      "Reward:  11.0\n",
      "Mean Reward 20.11255411255411\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  231\n",
      "Reward:  14.0\n",
      "Mean Reward 20.086206896551722\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  232\n",
      "Reward:  28.0\n",
      "Mean Reward 20.120171673819744\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  233\n",
      "Reward:  20.0\n",
      "Mean Reward 20.11965811965812\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  234\n",
      "Reward:  17.0\n",
      "Mean Reward 20.106382978723403\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  235\n",
      "Reward:  24.0\n",
      "Mean Reward 20.122881355932204\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  236\n",
      "Reward:  16.0\n",
      "Mean Reward 20.10548523206751\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  237\n",
      "Reward:  16.0\n",
      "Mean Reward 20.08823529411765\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  238\n",
      "Reward:  13.0\n",
      "Mean Reward 20.05857740585774\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  239\n",
      "Reward:  13.0\n",
      "Mean Reward 20.029166666666665\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  240\n",
      "Reward:  23.0\n",
      "Mean Reward 20.04149377593361\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  241\n",
      "Reward:  26.0\n",
      "Mean Reward 20.06611570247934\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  242\n",
      "Reward:  11.0\n",
      "Mean Reward 20.02880658436214\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  243\n",
      "Reward:  27.0\n",
      "Mean Reward 20.057377049180328\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  244\n",
      "Reward:  14.0\n",
      "Mean Reward 20.03265306122449\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  245\n",
      "Reward:  15.0\n",
      "Mean Reward 20.01219512195122\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  246\n",
      "Reward:  15.0\n",
      "Mean Reward 19.991902834008098\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  247\n",
      "Reward:  15.0\n",
      "Mean Reward 19.971774193548388\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  248\n",
      "Reward:  19.0\n",
      "Mean Reward 19.967871485943775\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  249\n",
      "Reward:  18.0\n",
      "Mean Reward 19.96\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  250\n",
      "Reward:  13.0\n",
      "Mean Reward 19.932270916334662\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  251\n",
      "Reward:  38.0\n",
      "Mean Reward 20.003968253968253\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  252\n",
      "Reward:  24.0\n",
      "Mean Reward 20.0197628458498\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  253\n",
      "Reward:  18.0\n",
      "Mean Reward 20.011811023622048\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  254\n",
      "Reward:  20.0\n",
      "Mean Reward 20.011764705882353\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  255\n",
      "Reward:  15.0\n",
      "Mean Reward 19.9921875\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  256\n",
      "Reward:  22.0\n",
      "Mean Reward 20.0\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  257\n",
      "Reward:  21.0\n",
      "Mean Reward 20.003875968992247\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  258\n",
      "Reward:  24.0\n",
      "Mean Reward 20.01930501930502\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  259\n",
      "Reward:  32.0\n",
      "Mean Reward 20.065384615384616\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  260\n",
      "Reward:  30.0\n",
      "Mean Reward 20.103448275862068\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  261\n",
      "Reward:  26.0\n",
      "Mean Reward 20.125954198473284\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  262\n",
      "Reward:  33.0\n",
      "Mean Reward 20.174904942965778\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  263\n",
      "Reward:  17.0\n",
      "Mean Reward 20.16287878787879\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  264\n",
      "Reward:  27.0\n",
      "Mean Reward 20.18867924528302\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  265\n",
      "Reward:  29.0\n",
      "Mean Reward 20.221804511278197\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  266\n",
      "Reward:  13.0\n",
      "Mean Reward 20.194756554307116\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  267\n",
      "Reward:  17.0\n",
      "Mean Reward 20.182835820895523\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  268\n",
      "Reward:  30.0\n",
      "Mean Reward 20.219330855018587\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  269\n",
      "Reward:  27.0\n",
      "Mean Reward 20.244444444444444\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  270\n",
      "Reward:  15.0\n",
      "Mean Reward 20.225092250922508\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  271\n",
      "Reward:  11.0\n",
      "Mean Reward 20.191176470588236\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  272\n",
      "Reward:  17.0\n",
      "Mean Reward 20.17948717948718\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  273\n",
      "Reward:  19.0\n",
      "Mean Reward 20.175182481751825\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  274\n",
      "Reward:  10.0\n",
      "Mean Reward 20.138181818181817\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  275\n",
      "Reward:  10.0\n",
      "Mean Reward 20.10144927536232\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  276\n",
      "Reward:  34.0\n",
      "Mean Reward 20.15162454873646\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  277\n",
      "Reward:  38.0\n",
      "Mean Reward 20.215827338129497\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  278\n",
      "Reward:  33.0\n",
      "Mean Reward 20.261648745519715\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  279\n",
      "Reward:  26.0\n",
      "Mean Reward 20.28214285714286\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  280\n",
      "Reward:  25.0\n",
      "Mean Reward 20.298932384341636\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  281\n",
      "Reward:  13.0\n",
      "Mean Reward 20.27304964539007\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  282\n",
      "Reward:  20.0\n",
      "Mean Reward 20.27208480565371\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  283\n",
      "Reward:  20.0\n",
      "Mean Reward 20.27112676056338\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  284\n",
      "Reward:  12.0\n",
      "Mean Reward 20.242105263157896\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  285\n",
      "Reward:  33.0\n",
      "Mean Reward 20.286713286713287\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  286\n",
      "Reward:  12.0\n",
      "Mean Reward 20.257839721254356\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  287\n",
      "Reward:  28.0\n",
      "Mean Reward 20.28472222222222\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  288\n",
      "Reward:  15.0\n",
      "Mean Reward 20.26643598615917\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  289\n",
      "Reward:  17.0\n",
      "Mean Reward 20.255172413793105\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  290\n",
      "Reward:  11.0\n",
      "Mean Reward 20.2233676975945\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  291\n",
      "Reward:  11.0\n",
      "Mean Reward 20.19178082191781\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  292\n",
      "Reward:  25.0\n",
      "Mean Reward 20.208191126279864\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  293\n",
      "Reward:  19.0\n",
      "Mean Reward 20.20408163265306\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  294\n",
      "Reward:  18.0\n",
      "Mean Reward 20.196610169491525\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  295\n",
      "Reward:  20.0\n",
      "Mean Reward 20.195945945945947\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  296\n",
      "Reward:  21.0\n",
      "Mean Reward 20.198653198653197\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  297\n",
      "Reward:  13.0\n",
      "Mean Reward 20.174496644295303\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  298\n",
      "Reward:  27.0\n",
      "Mean Reward 20.19732441471572\n",
      "Max reward so far:  67.0\n",
      "==========================================\n",
      "Episode:  299\n",
      "Reward:  15.0\n",
      "Mean Reward 20.18\n",
      "Max reward so far:  67.0\n"
     ]
    }
   ],
   "source": [
    "allRewards = []\n",
    "total_rewards = 0\n",
    "maximumRewardRecorded = 0\n",
    "episode = 0\n",
    "episode_states, episode_actions, episode_rewards = [],[],[]\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for episode in range(max_episodes):\n",
    "        \n",
    "        episode_rewards_sum = 0\n",
    "\n",
    "        # Launch the game\n",
    "        state = env.reset()\n",
    "        \n",
    "        env.render()\n",
    "           \n",
    "        while True:\n",
    "            \n",
    "            # Choose action a, remember WE'RE NOT IN A DETERMINISTIC ENVIRONMENT, WE'RE OUTPUT PROBABILITIES.\n",
    "            action_probability_distribution = sess.run(action_distribution, feed_dict={input_: state.reshape([1,4])})\n",
    "            \n",
    "            action = np.random.choice(range(action_probability_distribution.shape[1]), p=action_probability_distribution.ravel())  # select action w.r.t the actions prob\n",
    "\n",
    "            # Perform a\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "\n",
    "            # Store s, a, r\n",
    "            episode_states.append(state)\n",
    "                        \n",
    "            # For actions because we output only one (the index) we need 2 (1 is for the action taken)\n",
    "            # We need [0., 1.] (if we take right) not just the index\n",
    "            action_ = np.zeros(action_size)\n",
    "            action_[action] = 1\n",
    "            \n",
    "            episode_actions.append(action_)\n",
    "            \n",
    "            episode_rewards.append(reward)\n",
    "            if done:\n",
    "                # Calculate sum reward\n",
    "                episode_rewards_sum = np.sum(episode_rewards)\n",
    "                \n",
    "                allRewards.append(episode_rewards_sum)\n",
    "                \n",
    "                total_rewards = np.sum(allRewards)\n",
    "                \n",
    "                # Mean reward\n",
    "                mean_reward = np.divide(total_rewards, episode+1)\n",
    "                \n",
    "                \n",
    "                maximumRewardRecorded = np.amax(allRewards)\n",
    "                \n",
    "                print(\"==========================================\")\n",
    "                print(\"Episode: \", episode)\n",
    "                print(\"Reward: \", episode_rewards_sum)\n",
    "                print(\"Mean Reward\", mean_reward)\n",
    "                print(\"Max reward so far: \", maximumRewardRecorded)\n",
    "                \n",
    "                # Calculate discounted reward\n",
    "                discounted_episode_rewards = discount_and_normalize_rewards(episode_rewards)\n",
    "                                \n",
    "                # Feedforward, gradient and backpropagation\n",
    "                loss_, _ = sess.run([loss, train_opt], feed_dict={input_: np.vstack(np.array(episode_states)),\n",
    "                                                                 actions: np.vstack(np.array(episode_actions)),\n",
    "                                                                 discounted_episode_rewards_: discounted_episode_rewards \n",
    "                                                                })\n",
    "                \n",
    " \n",
    "                                                                 \n",
    "                # Write TF Summaries\n",
    "                summary = sess.run(write_op, feed_dict={input_: np.vstack(np.array(episode_states)),\n",
    "                                                                 actions: np.vstack(np.array(episode_actions)),\n",
    "                                                                 discounted_episode_rewards_: discounted_episode_rewards,\n",
    "                                                                    mean_reward_: mean_reward\n",
    "                                                                })\n",
    "                \n",
    "               \n",
    "                writer.add_summary(summary, episode)\n",
    "                writer.flush()\n",
    "                \n",
    "            \n",
    "                \n",
    "                # Reset the transition stores\n",
    "                episode_states, episode_actions, episode_rewards = [],[],[]\n",
    "                \n",
    "                break\n",
    "            \n",
    "            state = new_state\n",
    "        \n",
    "        # Save Model\n",
    "        if episode % 100 == 0:\n",
    "            saver.save(sess, \"./models/model.ckpt\")\n",
    "            print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Try Out Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/model.ckpt\n",
      "****************************************************\n",
      "EPISODE  0\n",
      "Score 52.0\n",
      "****************************************************\n",
      "EPISODE  1\n",
      "Score 16.0\n",
      "****************************************************\n",
      "EPISODE  2\n",
      "Score 12.0\n",
      "****************************************************\n",
      "EPISODE  3\n",
      "Score 21.0\n",
      "****************************************************\n",
      "EPISODE  4\n",
      "Score 28.0\n",
      "****************************************************\n",
      "EPISODE  5\n",
      "Score 17.0\n",
      "****************************************************\n",
      "EPISODE  6\n",
      "Score 14.0\n",
      "****************************************************\n",
      "EPISODE  7\n",
      "Score 18.0\n",
      "****************************************************\n",
      "EPISODE  8\n",
      "Score 13.0\n",
      "****************************************************\n",
      "EPISODE  9\n",
      "Score 9.0\n",
      "Score over time: 20.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    env.reset()\n",
    "    rewards = []\n",
    "    \n",
    "    # Load the model\n",
    "    saver.restore(sess, \"./models/model.ckpt\")\n",
    "\n",
    "    for episode in range(10):\n",
    "        state = env.reset()\n",
    "        env.render()\n",
    "        step = 0\n",
    "        done = False\n",
    "        total_rewards = 0\n",
    "        print(\"****************************************************\")\n",
    "        print(\"EPISODE \", episode)\n",
    "        while True:\n",
    "            \n",
    "\n",
    "            # Choose action a, remember WE'RE NOT IN A DETERMINISTIC ENVIRONMENT, WE'RE OUTPUT PROBABILITIES.\n",
    "            action_probability_distribution = sess.run(action_distribution, feed_dict={input_: state.reshape([1,4])})\n",
    "            #print(action_probability_distribution)\n",
    "            action = np.random.choice(range(action_probability_distribution.shape[1]), p=action_probability_distribution.ravel())  # select action w.r.t the actions prob\n",
    "\n",
    "\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "\n",
    "            total_rewards += reward\n",
    "\n",
    "            if done:\n",
    "                rewards.append(total_rewards)\n",
    "                print (\"Score\", total_rewards)\n",
    "                break\n",
    "            state = new_state\n",
    "    env.close()\n",
    "    print (\"Score over time: \" +  str(sum(rewards)/10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonRL",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
